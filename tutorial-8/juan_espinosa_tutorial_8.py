# -*- coding: utf-8 -*-
"""Juan Espinosa - tutorial 8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vbZypF-mBYyZpNVntnGXV30XPBnJwvu3

# MACHINE LEARNING LAB - TUTORIAL 8
---
## Juan Fernando Espinosa
## 303158

---

### In this homework the objective is to replicate the model developed by the authors Yi Zheng et. al. This model is applying Convolutional Neural Networks (CNN) mixed with a Multi-Layer Perceptron (MLP) to make predictions considering a multivariate time series.

# 1. ARCHITECTURE OF THE MODEL TO ACCOMPLISH THE RESULT


___

## Brief summary

> First, the input information is a multivariate time series, which means having more than one variable as input signals.

> In the pre-processing step it is necessary to treat each variable as univariate and perform feature learning. Normalize the dataset, apply sliding window, and cross-validation (leave-one-out).

> Second, the information after the pre-processing steps goes to a 2-stages CNN with the goal of extracting features.

> The first convolutional layer contains 8 kernels of size 5, after a subsampling process made by average pooling of size 2 and an activation function calculated by applying Sigmoid function. 

> The second convolutional with a purpose to help the previous one in the process of extracting features but deeper ones is formed by 4 kernels of size 5, and similarly as the first one with an average pooling and an activation function. 

> The MLP is formed by 1 hidden layer and takes as input 732 nodes, and gives an output of 4 (there are 4 predicted classes).

## Deep explanation and visualization flow of the process of the model

> After importing the data, concatenating and splitting into train and test the result will be a matrix similar to the image below on the left. 

* **Note:** The red line states that subject 7 has been taken as the test set.

> Next step is sliding window: the box slides down considering 2 factors: **Slide window** that represent the number of datapoints to take and **sliding step** which represents the number of datapoints the windows is going to slide in order to take another group of datapoints. 

* In the image below it is represented how the sliding window works: first it picks 256 datapoints and the window slides a given number (5 options were given in the paper) to take another 256 datapoints. Since above mentioned, the multivariate time series must be treaten as a univariate one, therefore, we slide over one label only. The same process applies for all the columns of the matrix.

![process](https://drive.google.com/uc?id=1YmkKxuQwV9u56x1MCnH27PabnQ-RKbKu)

> The output will be a 3D Tensor covering 9 channels (1 per column), 256 datapoints per row and the length of the tensor (L x C x R).

> Each channel is then used as the input of the CNN. Each row of the channel has to go through the 8 kernels of size 5 in the first stage. Of course, bearing into account the Average Pooling and Activation Function

![texto alternativo](https://drive.google.com/uc?id=1LpFtOF3FMyZP8c1a9a0cmhrnC3ZZg8w7)

![texto alternativo](https://drive.google.com/uc?id=1mcSBq9Xh1lk_I5td-qFa9G_cu2Ck1E8u)

> The output of the CNN then it is flatten to transform it into a 1d array. The length of the 1d array is now the input of the MLP and the output must be 4 according to the number of activities selected in the paper.

![texto alternativo](https://drive.google.com/uc?id=1apZpTopKq1fPfh0hN3PuMPy-6U1jZFDg)

> **Backpropagation:** The goal is to minimize the loss function. Since we have 4 activities CrossEntropy function is used. 

> To minimize the loss function it is required to get the derivative of the loss function considering the gradients of the parameters for a final update of those parameters. It is important to mention for the parameters update the SGD algorithm is used.

# MODEL IMPLEMENTATION


___

# 2 PRE-PROCESS GIVEN DATASETS
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
# %tensorflow_version 2.x
import tensorflow
import torchvision
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import torchvision.datasets as dataset
import torchvision.transforms as transforms
import torch.utils.data as Data
from torch.autograd import Variable
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
import pandas as pd
import numpy as np
from numpy import random
# %matplotlib inline
import math
import matplotlib.pyplot as plt
from google.colab import files
from google.colab import drive

# Commented out IPython magic to ensure Python compatibility.
print('PyTorch version:', torch.__version__)
print('Tensor version:',tensorflow.__version__)
# %load_ext tensorboard

drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject101.dat"
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject102.dat"
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject103.dat"
drive.mount('/content/drive')
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject104.dat"
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject105.dat"
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject106.dat"
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject107.dat"

"""---

> The very beginning of the pre-processing step is to upload all the subjects' information with the exception of subjects 8 and 9: one of them has left hand dominance which changes the outputs and subject 9 does not have data for the crucial metrics. 

> As a next step a concatenation of all data takes place.


---
"""

missing_values = ['-','na','Nan','nan','n/a','?']
column_names = ['Timestamp','Activity','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','43','44','45','46','47','48','49','50','51','52','53','54']
subject_1 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject101.dat", sep=' ', names = column_names)
subject_2 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject102.dat", sep=' ', names = column_names)
subject_3 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject103.dat", sep=' ', names = column_names)
subject_4 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject104.dat", sep=' ', names = column_names)
subject_5 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject105.dat", sep=' ', names = column_names)
subject_6 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject106.dat", sep=' ', names = column_names)
subject_7 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/LAB/tutorial 8/PAMAP2_Dataset/Protocol/subject107.dat", sep=' ', names = column_names)

subject_1['55'] = 101
subject_2['55'] = 102
subject_3[54] = 103
subject_4[54] = 104
subject_5[54] = 105
subject_6[54] = 106
subject_7['55'] = 107

frames = [subject_1, subject_2, subject_3, subject_4, subject_5, subject_6] 
data = pd.concat(frames)

"""## 2.1 SPLIT TRAIN AND TEST SET

### Train set

> Considering the dataset given by blocks of subjects' information to keep the order (it is a timeseries database) subject 7 is going to be chosen as the **test set**

> The columns chosen for this exercise are the following: COnsidering data of the first accelerometer (+16g) for each of the datas is recommended in the dataset information. Consequently, those 9 columns were selected.
"""

data = data[data['Activity'].isin([3, 4, 12, 13])]
data = data[['Activity', '5', '6', '7', '21', '22', '23', '39', '40', '41']] 
data.rename(columns={'5':'a1','6':'a2','7':'a3', '21':'b1', '22':'b2', '23':'b3', '39':'c1', '40':'c2', '41':'c3'}, inplace=True)
data.head()

"""### Test set"""

test = subject_7[subject_7['Activity'].isin([3, 4, 12, 13])]
test = test[['Activity', '5', '6', '7', '21', '22', '23', '39', '40', '41']] 
test.rename(columns={'5':'a1','6':'a2','7':'a3', '21':'b1', '22':'b2', '23':'b3', '39':'c1', '40':'c2', '41':'c3'}, inplace=True)
test.head()

"""## 2.2 NORMALIZATION

Following the instructions of the paper, the normalization method applied is by using the mean and standard deviation.

### Train set
"""

def normalize(dataset):
    datanorm = (dataset - dataset.mean())/dataset.std()
    datanorm["Activity"]= dataset["Activity"]
    #datanorm["subject"]= dataset["subject"]
    return datanorm

data_train = normalize(data)
data_train.head()

# DROP MISSING VALUES

missing_values = ['-','na','Nan','nan','n/a','?']
data_train = data.dropna()
data_train.head()

y_train = pd.get_dummies(data_train['Activity']).values

X_train = data_train.drop(['Activity'], axis=1).values
X_train.shape, y_train.shape

"""### Test set"""

test = normalize(test)
test.head()


# DROP MISSING VALUES

missing_values = ['-','na','Nan','nan','n/a','?']
test = test.dropna()
test.head()

y_test = pd.get_dummies(test['Activity'])
y_test = np.argmax(np.array(y_test), axis=1)
X_test = test.drop(['Activity'], axis=1)
X_test.shape, y_test.shape

"""## 2.3 SLIDING WINDOW

Following the above explanation, the process is applied directly to all columns.
"""

def window(X,y,step_size):
    sliding_window = 256  #number of datapoints to take
    length_y = len(y)  # Length of target
    sliding_step = length_y // step_size   # Helps to identify the sliding over the variable.
    X_array = []
    y_array = []

    for i in range(0, sliding_step):
        start = i * step_size
        end = sliding_window + (i * step_size)
        if end > length_y:
            start = length_y - sliding_window
            end = length_y
        X_array.append(np.array(X[start:end]))
        a = y[start:end]
        y_array.append(np.array(a.sum() / len(a)))
    X_array = np.array(X_array)
    Y_array = np.array(y_array)
    return X_array, Y_array

# Source = [5]

x_tr, y_tr = window(X_train, y_train, 128)
x_tr = np.moveaxis(x_tr, [0, 1, 2], [0, -1, -2])
x_tr.shape, y_tr.shape

x_te, y_te = window(X_test, y_test, 128)
x_te = np.moveaxis(x_te, [0, 1, 2], [0, -1, -2])
x_te.shape, y_te.shape

"""# 3. CNN and MLP

## 3.1 TRANSFORMATION TO TENSORS
"""

# Training
x_tr = torch.tensor(x_tr, dtype=torch.float)
y_tr = torch.tensor(y_tr, dtype=torch.long)

# Test
x_te = torch.tensor(x_te, dtype=torch.float)
y_te = torch.tensor(y_te, dtype=torch.long)

# Joining X and Y together in a dataset to ease the prediction process.
datasets_train = torch.utils.data.TensorDataset(x_tr, y_tr)
datasets_test = torch.utils.data.TensorDataset(x_te, y_te)

# Dataloaders for train and test. 
train_loader = Data.DataLoader(dataset=datasets_train, batch_size=50, shuffle=False, sampler=None, batch_sampler=None)
test_loader = Data.DataLoader(dataset=datasets_test, batch_size=1, shuffle=False)
x_tr.shape, y_tr.shape, x_te.shape, y_te.shape,

"""## 3.2 CNN STRUCTURE

2 Stages:


1.   first: 8 kernels of size 5.
2.   second: 4 kernels of size 5.
3.   Sigmoid function
4.   Average pooling.

> MLP:



1.   Input:
2.   Output of 1 hidden layer: 100
3.   input: 100
4.   Final Output: 4
"""

class TwoLayerNet(torch.nn.Module):
    def __init__(self):
        super(TwoLayerNet,self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv1d(9, 8, kernel_size=5),  
            nn.Sigmoid(),
            nn.AvgPool1d(kernel_size=2, stride=2))
        self.conv2 = nn.Sequential(
            nn.Conv1d(8, 4, kernel_size=5),
            nn.Sigmoid(),
            nn.AvgPool1d(kernel_size=2, stride=2))

        self.fc1 = nn.Linear(244, 100) 
        self.fc2 = nn.Linear(100, 4)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        out = out.view(out.size(0), -1)
        out = self.fc1(out)
        out = self.fc2(out)
        return out

# Source = [3]

model = TwoLayerNet()
print(model)

# Optimizer and stablishing the loss function as CrossEntropy
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
loss = torch.nn.CrossEntropyLoss()

epochs = 200
loss_array = []
accuracy_array = []
writer = SummaryWriter('./test5')
total_step = len(train_loader)
for epoch in range(epochs):
  print('---- Epoch:', epoch, ' ----')
  loss_epoch = 0.0
  losses = 0.0
  correct = 0.0
  total_size = 0.0
  accuracy = 0.0
  accuracy_epoch = 0.0
  #print("{Epoch}", epoch)
  for i, (batch_X, batch_y) in enumerate(train_loader, 0):    # for each training step

      X_temporal, y_temporal = Variable(batch_X), Variable(batch_y)

      # It is required to clear the gradients 
      optimizer.zero_grad()

      #Forward pass - model prediction
      outputs = model(X_temporal.float())
      y_temporal = y_temporal.squeeze_()
      loss_size = loss(outputs, y_temporal)
      total_size += y_temporal.size(0)
      _, predicted = torch.max(outputs.data, 1)
      correct += (predicted == y_temporal).sum().item()
      accuracy = correct / total_size

      # Backpropagation
      loss_size.backward() 
       
      optimizer.step() 
      losses += loss_size.data

      
      
  loss_epoch = losses.item() / len(data)
  loss_array.append(losses.item() / len(data))
  accuracy_epoch = accuracy / len(data)   
  accuracy_array.append(accuracy / len(data)*100)
  print('Loss Trainig Set:', losses.item() / 50)  
  writer.add_scalar('Loss/train', losses.item(), epoch)
  writer.add_scalar('Accuracy/train', correct / total_size, epoch)




# test
  for i, (batch_X, batch_y) in enumerate(test_loader, 0):

      X_temporal, y_temporal = Variable(batch_X), Variable(batch_y)

      # It is required to clear the gradients 
      optimizer.zero_grad()

      #Forward pass - model prediction
      outputs = model(X_temporal.float())
      loss_size = loss(outputs, y_temporal)
      total_size += y_temporal.size(0)
      _, predicted = torch.max(outputs.data, 1)
      correct += (predicted == y_temporal).sum().item()
      accuracy = (correct / total_size)*100

      # Backpropagation
      loss_size.backward()  
      optimizer.step() 
      losses += loss_size.data

  accuracy_epoch = accuracy / len(data)   
  accuracy_array.append(accuracy / len(data)*100)
  print('Accuracy on Test set:', (correct / total_size) * 100, '\n')
  writer.add_scalar('Loss/test', losses.item(), epoch)
  writer.add_scalar('Accuracy/test', correct / total_size, epoch)
writer.close()
# Source: [3]

plt.plot(loss_array)
print(loss_array)
plt.show()

"""![texto alternativo](https://drive.google.com/uc?id=1CATVG8xHQFCO01sbLSoEozWnGtSavSFs)

# 4. CONCLUSIONS

> After 200 epochs the accuracy obtained in the test set is 99%. It is really high but it happens because of the number of iterations. If the number of iterations is reduced to 40, the dropping in accuracy is of around 8%. 

> The distribution of train and test seems to be not good and cross validation is fundamental for this exercises to avoid overfitting.  

> By using Adam for learning rate the algorithm runs faster and reaches a good aaccuracy. 

> Pytorch directly allows you to use minibatches. Therefore, if at the end minibatches are still used could be an opportunity to test results with different type of optimizer.

> It was not possible to replicate the results of the paper due to a not-well-stablished used of the columns. In this exercise, 9 columns were used. It generates a good accuracy percentage.

# 5. BIBLIOGRAPHY:



[1] Ackermann, N. (2018, September 14). Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences. Retrieved from https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf.


[2] Brownlee, J. (2019, August 5). How to Develop Convolutional Neural Network Models for Time Series Forecasting. Retrieved from https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/.

[3] Convolutional Neural Networks Tutorial in PyTorch. (2018, June 16). Retrieved from https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/.

[4] torch.utils.tensorboard¶. (n.d.). Retrieved from https://pytorch.org/docs/stable/tensorboard.html.

[5] Window Sliding Technique. (2019, October 25). Retrieved from https://www.geeksforgeeks.org/window-sliding-technique/.

[6] Zheng, Y., Liu, Q., Chen, E., Ge, Y., & Zhao, J. L. (2014, June). Time series classification using multi-channels deep convolutional neural networks. In International Conference on Web-Age Information Management (pp. 298-310). Springer, Cham.
"""

