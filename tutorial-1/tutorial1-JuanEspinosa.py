# -*- coding: utf-8 -*-
"""tutorial1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCfPZNAca89bUFIFuoPXhqGzbEZABj5b

# MACHINE LEARNING LAB - TUTORIAL 1
---
## Juan Fernando Espinosa
## 303158

# **`EXERCISE SHEET 1`**


---

1. First step is to read the database and transform it into a dataframe to make it possible to edit (add/erase,etc) columns of the database.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/My Drive/MASTER DATA ANALYTICS/SEMESTER 2/Lab ML/Grades.csv"

grades=pd.read_csv('/content/drive/My Drive/MASTER DATA ANALYTICS/SEMESTER 2/Lab ML/Grades.csv')
grade = pd.DataFrame(grades)

"""2. Check the columns for a better understanding of the database distribution."""

grade.columns
grade.drop("Final Grade", axis=1)
grade = grade.round()

"""3. Check if the database does not have empty fields."""

check = grade.empty
print(check)

"""## 1. 1\. PYTHON AND NUMPY

### 1. 1\. 1\. **Grading Program**
---

### 1. 1\. 1\. 1\. Sum of the grades for each student
the sum of the grades of each students considering the courses: English, Math. Science, German and Sports are allocated in a new column called **sum**
"""

grade['sum']=grade[['English', ' Maths', 'Science', 'German','Sports' ]].sum(axis=1)
grade
grade[['First Name','Last Name','English',' Maths', 'Science', 'German','Sports','sum']]

"""### 1. 1\. 1\. 2\. Average of the grades of each student
Similar to the previous exercise, a new column is created called **Average**
"""

grade['Average']=grade[['English', ' Maths', 'Science', 'German','Sports' ]].mean(axis=1)
grade.Average = grade.Average.round()
grade[['First Name','Last Name','English',' Maths', 'Science', 'German','Sports','sum','Average']]

"""### 1. 1\. 1\. 3\. Standard deviation for the grades for each student 
the new column is called **STD**
"""

grade['STD']=grade[['English', ' Maths', 'Science', 'German','Sports' ]].std(axis=1)
grade
grade[['First Name','Last Name','English',' Maths', 'Science', 'German','Sports','sum','Average', 'STD']]

"""### 1. 1\. 1\. 4\. Plot the average points of all the students
For plotting the graph, the columns used are: *first name* and average
"""

rng = np.random.RandomState(0)
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
colors1 = 'blue'
plt.rcParams["figure.figsize"] = fig_size
plt.ylabel('Average')
plt.xlabel('Names')
plt.scatter(grade['First Name'], grade['Average'], c=colors1)
plt.show()

"""### 1. 1\. 1\. 5\. Assign a grade based on the following rubric
 
In consideration to the image shared, several conditions have to me met to assign each student to the ideal grade.
"""

conditions = [
    (grade['Average'] >= 96.0),
    (grade['Average'] >= 90.0) & (grade['Average'] < 96.0),
    (grade['Average'] >= 86.0) & (grade['Average'] < 90.0),
    (grade['Average'] >= 80.0) & (grade['Average'] < 86.0),
    (grade['Average'] >= 76.0) & (grade['Average'] < 80.0),
    (grade['Average'] >= 70.0) & (grade['Average'] < 76.0),
    (grade['Average'] >= 66.0) & (grade['Average'] < 70.0),
    (grade['Average'] >= 60.0) & (grade['Average'] < 66.0),
    (grade['Average'] >= 56.0) & (grade['Average'] < 60.0),
    (grade['Average'] >= 0.0) & (grade['Average'] <= 55.0)]
gradingSystem = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'D', 'F']
grade['finalGrade'] = np.select(conditions, gradingSystem)

grade[['First Name','Last Name','STD','finalGrade']]

"""### 1. 1\. 1\. 6\. Plot the histogram of the final grades"""

grade['count'] = grade[['finalGrade']].count(axis=1)

studentsGrades = grade.pivot_table(index=['finalGrade'], aggfunc='size')
print(studentsGrades)

fig, ax = plt.subplots(figsize=(15,6))
plt.xlabel('Final Grade')
plt.ylabel('Count of students')
grade.groupby(['finalGrade']).count()['count'].plot(kind='bar',title="Final Grades")

"""## 1. 1\. 2\. Matrix Multiplication
---

First, intialization of the matrix **A** considering the mean and standard deviation. Moreover, adding a random vector **v**
"""

import numpy as np 
A = np.random.normal(loc =5, scale =0.01, size=(100, 20))
v = np.random.normal(loc =5, scale =0.01, size=(20, 1))

"""### 1. 1\. 2\. 1\. Iterative multiply (element-wise) each row of matrix A with vector v and sum the result of each iteration in another vector **C**"""

c = []
for i in range(len(A)):
  rowA = A[i]
  for j in range(len(v[0])):
    result = 0
    for k in range(len(v)):
      numA = rowA[k]
      numB = v[k,j]
      result += numA * numB
  c.append(result)
print('vector c:',[c])

"""### 1. 1\. 2\. 2\. Find the mean and Standard deviation of vector **v**

Mean:
"""

def mean(xs: [float]):
    return sum(xs) / len(xs)

print('Mean:',mean(c))

"""Standard deviation:"""

import math
suma = 0

for i in range(len(c)):
  numerator = (c[i] - mean(c))**2
  suma += numerator
  denominator = len(c) - 1
  division = suma / denominator
  total = math.sqrt(division)
print('Standard Deviation:',total)

"""### 1. 2\. 2\. 3\. Plot histogram of vector **c** with 5 bins"""

fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size
plt.hist(c, 5)
plt.show()

"""## 1. 2\. Linear Regression through exact form

## 1. 2\. 1\. OLS
---

### 1. 2\. 1\. 1\. Implement Matrix A with random values.

First, initilization of a random matrix **A** with the dimensions, mean, and standard deviation assigned.
"""

A1 = np.random.normal(loc =2, scale =1, size=(100, 2))
A1

"""### 1. 2\. 1\. 2\. Implement Learn Simple LIN-REG to find values of $Beta_0$ and $Beta_1$.

In order to solve the linear regression is important to bear in mind the formula to get the Least Squares. 
>$y-pred=\sum_{i=0}^n \frac{(x - \overline{x})(y - \overline{y})}{(x - \overline{x
})^2}$

* The first step made was to transpose matrix **A1** with the goal of making it easier to iterate with the values in the columns in matrix A1. 
* Second, finding the mean of each column in matrix **A1**, values representing $\overline{x}$ and $\overline{y}$.
* Third, splitting the formula in 2: Numerator and denominator for a cleaner calculation. 
* Finally, Applying the following formulas by using the outputs of the previous step: numerator and denominator.

> $y = B_0 + B1x$



> $B_0 = \overline{y} - B_1\overline{x}$


> $B_1 = \overline{y} - B_0\overline{x}$
"""

# Transpose of A1
A2 = A1.T

# Mean of each column in Matrix A1
MeanA = mean(A1)
print('Mean A:', MeanA)


# Calculus of the numerator of the formula 
numerator = (A2[0] - MeanA[0])*(A2[1] - MeanA[1])
totalNum = 0
for i in numerator:
  totalNum += i

# Calculus of the denominator of the formula
denominator = (A2[0] - MeanA[0])**2
totalDen = 0
for i in denominator:
    totalDen += i

# Calculus of the slope and y-intercept for the Linear Regression
beta1 = totalNum / totalDen
beta0 = MeanA[1] - (beta1*(MeanA[0]))
print('Beta0:', beta0)
print('Beta1:', beta1)

"""### 1. 2\. 1\. 3\. SIMPLE-LINREG for all data points (predictions)"""

y_prediction = []
f = beta1*A2[0] + beta0
y_prediction = f
print("y_prediction:", y_prediction)

"""### 1. 2\. 1\. 4\. Plot the training data and the predicted line."""

fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size

plt.scatter(A2[0], A2[1], color='green', alpha=0.5)
plt.plot(A2[0],y_prediction, c ='blue')
plt.xlabel('X values')
plt.ylabel('Y values')
plt.show()

"""**Observations**: considering the slope and y-intercept above calculated ($Beta_0$ and $Beta_1$) the relationship between the 2 variables is as follow: For every aditional increase in value in the X axis the mean of the y-values will decrease in exactly the value of $B_1$ considering the error $B_0$.

### 1. 2\. 1\. 5\. $B_1 = 0$ and re-run the predicted line
"""

y1_prediction = []
f1 = 0*A2[0] + beta0
y1_prediction = f1
print("y1_prediction:", y1_prediction)

fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size

plt.scatter(A2[0], A2[1], color='green', alpha=0.5)
plt.xlabel('X values')
plt.ylabel('y values')
plt.plot(A2[0],y1_prediction, c ='blue')
plt.show()

"""**Observation:** A slope equals to zero means that the mean of the dependant variable is not going to change no matter how much the values of the X-axis increases or decreases. It means that there is a high p-value to conclude insignificance of the scenario (changes in the predictor value does not change the result in the response variable).

### 1. 2\. 1\. 6\. $B_0 = 0$ and re-run the predicted line
"""

y2_prediction = []
f2 = beta1*A2[0] + 0
y2_prediction = f2
print("y2_prediction:", y2_prediction)

fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size
plt.scatter(A2[0], A2[1], color='green', alpha=0.5)
plt.xlabel('X values')
plt.ylabel('y values')
plt.plot(A2[0],y2_prediction, c ='blue')
plt.show()

"""***Observations:*** The y-intercept is the value at which the fitted line cross the y-axis. The constant guarantees that the result will not have positive or negative bias. It absorbs the bias existent in the relationship between the independant and dependant variable. If $B_0$ is forced to be zero, the regression must to go through the origin, if not some bias has been bearing into account when calculating the *slope* and *y-predictions*. As we can see in the image, without a y-intercept the predictions for a few datapoints tend to be high and low for others. The regression does not capture the esence of the dataset and could infere **underfitting** (not capturing the underlying structure of the data).

### 1. 2\. 1\. 7\. Use numpy.linalg lstsq and replace steps 2
"""

v=[]
for i in A1:
  v+=[[i[0],1]]
B = np.linalg.lstsq(v, A1[:,1], rcond=None)[0]
print('Betas',B)

"""Since the Numpy Linalg Lstsq minimizes X in regards to $||ax - b||_2^2$. It uses a Singular Value Decomposition to solve least squares. Basically it finds a solution where $ |x|_2$ is minimized by finding the minimum norm solution. But also finds the minimum norm least squares for for x : $||ax - b||_2$. In addition, this formula also calculates the rank of the matrix, the residuals and the error.

## 1. 2\. 2\. OLS using real dataset
---

The first step is to import the database and add the columns' names because the file does not explicitly show them. Moreover, reading the description and head of the dataset gives a better understanding of the structure of it.
"""

drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/auto-mpg.data"

column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']
missing_values = ['-','na','Nan','nan','n/a','?']
MPG=pd.read_csv('/content/drive/My Drive/Colab Notebooks/auto-mpg.data', delim_whitespace=True, names=column_names, na_values = missing_values)
MPG.head()

"""Next, always check for missing or incongruent values in the dataset. By using *MPG.empty* a boolean answer is returned if empty fields appear in the dataset. Moreover, by using *.dtypes* the output facilitates to identify if the type is correct according to the values in the columns."""

check = MPG.empty
print('checking missing values:',check)
print('Sum of errors:',MPG.isnull().sum())

"""As we can see, the column horsepower has 6 incongruent values which make the dtype of the column to be an object. To debug the dataset, it is necessary to replace the values of those fields with the mean of the column."""

MPG['horsepower'] = MPG['horsepower'].fillna((MPG['horsepower'].mean()))
print('Sum of errors:',MPG.isnull().sum())

"""Moreover, as each column in the dataset has different range of values it is necessary to scale them and get an unique range of values."""

def scale(value):
    new = (value-value.min())/(value.max()-value.min())
    return new

MPG_scale = MPG.copy()

MPG_scale ['displacement'] = scale(MPG_scale['displacement'])
MPG_scale['horsepower'] = scale(MPG_scale['horsepower'])
MPG_scale ['acceleration'] = scale(MPG_scale['acceleration'])
MPG_scale ['weight'] = scale(MPG_scale['weight'])
MPG_scale['mpg'] = scale(MPG_scale['mpg'])

"""### 1. 2\. 2\. 1\. Split the database into a uni-variate case with Displacement and mpg as $x$ and $y$."""

MPG_univariate = MPG_scale[['displacement', 'mpg']]
print(MPG_univariate)

"""### 1. 2\. 2\. 2\. Find $B_0$ and $B_1$."""

# Calculus of the Mean for each column
mean_univariate = MPG_univariate.mean(axis = 0)
Mean_MPG_univariate = mean_univariate[0]
Mean_MPG_univariate1 = mean_univariate[1]
print('Mean x:', Mean_MPG_univariate)
print('Mean y:', Mean_MPG_univariate1)

# Split the formula in 2: numerator and denominator for an ease understanding
numerator = (MPG_univariate['displacement'] - Mean_MPG_univariate)*(MPG_univariate['mpg'] - Mean_MPG_univariate1)
totalNum = 0
for i in numerator:
  totalNum += i

denominator = (MPG_univariate['displacement'] - Mean_MPG_univariate)**2
totalDen = 0
for i in denominator:
    totalDen += i

# Calculus of the Betas
beta_mpg1 = totalNum / totalDen
beta_mpg0 = Mean_MPG_univariate1 - (beta_mpg1*(Mean_MPG_univariate))
print('Beta0:', beta_mpg0)
print('Beta1:', beta_mpg1)

"""### 1. 2\. 2\. 3\. $y-prediction$: calculate the points for each training example in A."""

y_prediction3 = []
p = beta_mpg1*MPG_univariate['displacement'] + beta_mpg0
y_prediction3 = p
print("y_prediction:", y_prediction3)

"""### 1. 2\. 2\. 4\. Plot the training data and the predicted line."""

fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size
plt.scatter(MPG_univariate['displacement'], MPG_univariate['mpg'], color='green', alpha=0.5)
plt.xlabel('Displacement')
plt.ylabel('mpg')
plt.plot(MPG_univariate['displacement'],y_prediction3, c ='blue')
plt.show()

"""**Observations:** A vehicle *displacement* affects the Mile per Gallon a car needs. For each increase in displacement, the consumption of fuel will increase in 0,62 considering the scale previously created.

### 1. 2\. 2\. 5\. Repeat the above but change the indepedant variable to ["horsepower","weight","acceleration"],
* $x:$ horsepower |  $y:$ mpg
"""

MPG_horsepower = MPG_scale[['horsepower', 'mpg']]



# Calculus of the Mean for each column
mean_horsepower = MPG_horsepower.mean(axis = 0)
Mean_MPG_horsepower_x = mean_horsepower[0]
Mean_MPG_horsepower_y = mean_horsepower[1]
print('Mean x:', Mean_MPG_horsepower_x)
print('Mean y:', Mean_MPG_horsepower_y)

numerator = (MPG_horsepower['horsepower'] - Mean_MPG_horsepower_x)*(MPG_horsepower['mpg'] - Mean_MPG_horsepower_y)
totalNum = 0
for i in numerator:
  totalNum += i

denominator_horsepower = (MPG_horsepower['horsepower'] - Mean_MPG_horsepower_x)**2
totalDen = 0
for i in denominator:
    totalDen += i


# Calculus of the Betas
beta_horsepower_1 = totalNum / totalDen
beta_horsepower_0 = Mean_MPG_horsepower_y - (beta_horsepower_1*(Mean_MPG_horsepower_x))
print('Beta0:', beta_horsepower_0)
print('Beta1:', beta_horsepower_1)

# Prediction of y for all datapoints in X.
y_prediction_horsepower = []
p1 = beta_horsepower_1*MPG_horsepower['horsepower'] + beta_horsepower_0
y_prediction_horsepower = p1
print("y_prediction:", y_prediction_horsepower)


#Plotting the graph considering the Betas found and the predictions.
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size

plt.scatter(MPG_horsepower['horsepower'], MPG_horsepower['mpg'], color='green', alpha=0.5)
plt.xlabel('Horsepower')
plt.ylabel('mpg')
plt.plot(MPG_horsepower['horsepower'],y_prediction_horsepower, c ='blue')
plt.show()

"""**Observations:** The dependence of MPG in regards to Horsepower is negative considering the pursue of the dataset. Each increase per unit in Horsepower in a vehicle, results in a 0.57 decrease in gallons per mile. Therefore, more fuel is required to address the demand.

* $x:$ weight |  $y:$ mpg
"""

MPG_weight = MPG_scale[['weight', 'mpg']]


# Calculus of the Mean for each column
mean_weight = MPG_weight.mean(axis = 0)
Mean_MPG_weight_x = mean_weight[0]
Mean_MPG_weight_y = mean_weight[1]
print('Mean x:', Mean_MPG_weight_x)
print('Mean y:', Mean_MPG_weight_y)

numerator = (MPG_weight['weight'] - Mean_MPG_weight_x)*(MPG_weight['mpg'] - Mean_MPG_weight_y)
totalNum = 0
for i in numerator:
  totalNum += i

denominator = (MPG_weight['weight'] - Mean_MPG_weight_x)**2
totalDen = 0
for i in denominator:
    totalDen += i


# Calculus of the Betas
beta_weight_1 = totalNum / totalDen
beta_weight_0 = Mean_MPG_weight_y - (beta_weight_1*(Mean_MPG_weight_x))
print('Beta0:', beta_weight_0)
print('Beta1:', beta_weight_1)


# Prediction of y for all datapoints in X.
y_prediction_weight = []
p2 = beta_weight_1*MPG_weight['weight'] + beta_weight_0
y_prediction_weight = p2
print("y_prediction:", y_prediction_weight)


#Plotting the graph considering the Betas found and the predictions.
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size

plt.scatter(MPG_weight['weight'], MPG_weight['mpg'], color='green', alpha=0.5)
plt.xlabel('Weight')
plt.ylabel('mpg')
plt.plot(MPG_weight['weight'],y_prediction_weight, c ='blue')
plt.show()

"""**Obervations:** *weight* and *mpg* have a negative relationship. The heavier a vehicle is, the more gallons per mile it will need. In the presented graph, for each increase in 1 ton of a vehicle weight, the vehicle will need a 0,72 increase in gallons per mile. Therefore, the final output is an increase in fuel consumption.

* $x:$ Acceleration |  $y:$ mpg
"""

MPG_acceleration = MPG_scale[['acceleration', 'mpg']]

# Calculus of the Mean for each column
mean_acceleration = MPG_acceleration.mean(axis = 0)
print(mean_acceleration)
Mean_MPG_acceleration_x = mean_acceleration[0]
Mean_MPG_acceleration_y = mean_acceleration[1]
print('Mean x:', Mean_MPG_acceleration_x)
print('Mean y:', Mean_MPG_acceleration_y)

numerator = (MPG_acceleration['acceleration'] - Mean_MPG_acceleration_x)*(MPG_acceleration['mpg'] - Mean_MPG_acceleration_y)
totalNum = 0
for i in numerator:
  totalNum += i

denominator = (MPG_acceleration['acceleration'] - Mean_MPG_acceleration_x)**2
totalDen = 0
for i in denominator:
    totalDen += i


# Calculus of the Betas
beta_acceleration_1 = totalNum / totalDen
beta_acceleration_0 = Mean_MPG_acceleration_y - (beta_acceleration_1*(Mean_MPG_acceleration_x))
print('Beta0:', beta_acceleration_0)
print('Beta1:', beta_acceleration_1)


# Prediction of y for all datapoints in X.
y_prediction_acceleration = []
p3 = beta_acceleration_1*MPG_acceleration['acceleration'] + beta_acceleration_0
y_prediction_acceleration = p3
print("y_prediction:", y_prediction_acceleration)


#Plotting the graph considering the Betas found and the predictions. 
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size

plt.scatter(MPG_acceleration['acceleration'], MPG_acceleration['mpg'], color='green', alpha=0.5)
plt.xlabel('Acceleration')
plt.ylabel('mpg')
plt.plot(MPG_acceleration['acceleration'],y_prediction_acceleration, c ='blue')
plt.show()

"""The relationship between *acceleration* and *mpg* is positive. For every one porcentual point in acceleration increased, the mean value of miles per gallon will increase in a value of 0,53. In conclusion, the more acceleration a vehicle has, the less fuel consumption it will require.

*Sumary*
1. The faster a car goes, the less fuel consumption it makes.
2. The heavier the car, the less miles per hour it will get.
3. The more power a car needs to be moved (horsepower) the more fuel it requires to work optimally, increasing the consumption of fuel per mile.
4. The more effort is done by the displacement of the car, it requires more fuel to work, therefore, the MPG will increase.

**Final thought:**
The worst impact for a city (giving perspective of the dataset) depends on the weight of a car. It has the higher neccesity of fuel. On second place, the effort made by the engine requires sufficient fuel which speeds the fuel consumption. On the other hand, acceleration has a positive impact in the mpg-relationship. According to this analysis, decision-making could take place.

### 1. 2\. 2\. 6\. use scikit learn to implement OLS Linear Model.
"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()  

#Displacement and MPG

X = MPG_scale['displacement'].values.reshape(-1,1)
y = MPG_scale['mpg'].values.reshape(-1,1)
regressor = LinearRegression()
regressor.fit(X, y) #training the algorithm
#Retrieving the intercept:
print('regressor Intercept displacement',regressor.intercept_)
#Retrieving the slope:
print('regressor Coef displacement',regressor.coef_)
#Retrieving the predictions in regards to X:
y_OLS_predictions_displacement = regressor.predict(X)


#Horsepower and MPG

X1 = MPG_scale['horsepower'].values.reshape(-1,1)
y1 = MPG_scale['mpg'].values.reshape(-1,1)
regressor1 = LinearRegression()
regressor1.fit(X1, y1) #training the algorithm
#Retrieving the intercept:
print('regressor Intercept horsepower',regressor.intercept_)
#Retrieving the slope:
print('regressor Coef horsepower',regressor.coef_)
#Retrieving the predictions in regards to X:
y_OLS_predictions_horsepower = regressor.predict(X1)

# Weight and MPG

X2 = MPG_scale['weight'].values.reshape(-1,1)
y2 = MPG_scale['mpg'].values.reshape(-1,1)
regressor2 = LinearRegression()
regressor2.fit(X2, y2) #training the algorithm
#Retrieving the intercept:
print('regressor Intercept weight',regressor.intercept_)
#Retrieving the slope:
print('regressor Coef weight',regressor.coef_)
#Retrieving the predictions in regards to X:
y_OLS_predictions_weight = regressor.predict(X2)


# Acceleration and MPG

X3 = MPG_scale['acceleration'].values.reshape(-1,1)
y3 = MPG_scale['mpg'].values.reshape(-1,1)
regressor3 = LinearRegression()
regressor3.fit(X3, y3) #training the algorithm
#Retrieving the intercept:
print('regressor Intercept acceleration',regressor.intercept_)
#Retrieving the slope:
print('regressor Coef acceleration',regressor.coef_)
#Retrieving the predictions in regards to X:
y_OLS_predictions_acceleration = regressor.predict(X3)

"""### 1. 2\. 2\. 7\. Plot the training data (use plt.scatter) and your predicted line (use plt.plot)."""

# Displacement
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 4
fig_size[1] = 4
plt.rcParams["figure.figsize"] = fig_size
plt.scatter(X, y, color='green', alpha=0.5)
plt.xlabel('displacement')
plt.ylabel('mpg')
plt.plot(X,y_OLS_predictions_displacement, c ='blue')
plt.show()

# Horsepower
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 4
fig_size[1] = 4
plt.rcParams["figure.figsize"] = fig_size
plt.scatter(X1, y1, color='green', alpha=0.5)
plt.xlabel('horsepower')
plt.ylabel('mpg')
plt.plot(X1,y_OLS_predictions_horsepower, c ='blue')
plt.show()

# Weight
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 4
fig_size[1] = 4
plt.rcParams["figure.figsize"] = fig_size
plt.scatter(X2, y2, color='green', alpha=0.5)
plt.xlabel('weight')
plt.ylabel('mpg')
plt.plot(X2,y_OLS_predictions_weight, c ='blue')
plt.show()

# Acceleration
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 4
fig_size[1] = 4
plt.rcParams["figure.figsize"] = fig_size
plt.scatter(X3, y3, color='green', alpha=0.5)
plt.xlabel('acceleration')
plt.ylabel('mpg')
plt.plot(X3,y_OLS_predictions_acceleration, c ='blue')
plt.show()

"""# Bibliography

+ Devanshbesain. (2017, August 3). Exploration and analysis - Auto-MPG. Retrieved from https://www.kaggle.com/devanshbesain/exploration-and-analysis-auto-mpg.

+ Editor, M. B. (n.d.). Regression Analysis: How to Interpret the Constant (Y Intercept). Retrieved from https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-to-interpret-the-constant-y-intercept.

+ Editor, M. B. (n.d.). Why You Need to Check Your Residual Plots for Regression Analysis: Or, To Err is Human, To Err Randomly is Statistically Divine. Retrieved from https://blog.minitab.com/blog/adventures-in-statistics-2/why-you-need-to-check-your-residual-plots-for-regression-analysis.

+ Sarigoz, F. (n.d.). Fatih Sarigoz Data Science Blog. Retrieved from https://fatihsarigoz.com/tag/python-machine_learning-auto-mpg-dataset.html.

+ Editor, M. B. (n.d.). Why You Need to Check Your Residual Plots for Regression Analysis: Or, To Err is Human, To Err Randomly is Statistically Divine. Retrieved from https://blog.minitab.com/blog/adventures-in-statistics-2/why-you-need-to-check-your-residual-plots-for-regression-analysis.

+ sklearn.linear_model.LinearRegression¶. (n.d.). Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html.
"""

