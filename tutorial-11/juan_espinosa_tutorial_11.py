# -*- coding: utf-8 -*-
"""Juan Espinosa - tutorial 11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wg6IzVEy_bqVAPI5kuGAOOPrutN5hcXV

# MACHINE LEARNING LAB - TUTORIAL 11
---
## Juan Fernando Espinosa
## 303158

---

# DATA IMPORTING 

## The data chosen for this tutorial is **Movielens 100k movie ratings**.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import math
import matplotlib.pyplot as plt
from google.colab import files
from google.colab import drive
from sklearn import datasets 
from pandas.plotting import scatter_matrix

drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 11/iris1.csv"
iris = datasets.load_iris()

arrays = np.array([[0,0], 
                   [0,1],
                   [-1,2],
                   [2,0],
                   [3,0],
                   [4,-1]])
arrays.dtype

X = iris.data 
y = iris.target

"""# EXERCISE 1: K-MEANS"""

# IDENTIFYING THE K CENTROIDS

Cluster1 = np.array(X[0])
ClusterA = []
ClusterB = []
ClusterC = []
distances = []
new_list = []
Cluester_new = 0
for i in range(len(X)):
  distance =0
  for j in range(len(X[i])):
    distance += np.power((X[i][j] - Cluster1[j]),2)  
  distances.append(distance)
max_distance = np.amax(distances)
result = np.where(distances == max_distance)
distances.remove(max_distance)
second_max_distance = np.amax(distances)
result1 = np.where(distances == second_max_distance)
Cluster2 = X[result]
Cluster3 = X[result1]
Clusters = np.append([Cluster1], Cluster2,axis = 0)
Clusters = np.append(Clusters, Cluster3, axis = 0)
print('Initial Cluster',Clusters)
Distance_cluster = []

# INITIALIZATION OF THE K MEANS ALGORITHM
for _ in range(15):
  ClusterA = []
  ClusterB = []
  ClusterC = []
  for i in range(len(Clusters)):
    partial_distance = []
    for k in range(len(X)):
      distance =0
      for j in range(len(X[i])):
        distance += np.power((X[k][j] - Clusters[i][j]),2)
      partial_distance.append(distance)
    Distance_cluster.append(partial_distance)
  min_Values = np.argmin(Distance_cluster, axis=0)
  for i in range(len(min_Values)):
    if min_Values[i] == 0:
      ClusterA.append(X[i])
    elif min_Values[i] == 1:
      ClusterB.append(X[i])
    else:
      ClusterC.append(X[i])
  ClusterA = np.array(ClusterA)
  ClusterB = np.array(ClusterB)
  ClusterC = np.array(ClusterC)

  C1_mean = np.mean(ClusterA, axis=0)
  C2_mean = np.mean(ClusterB, axis = 0)
  C3_mean = np.mean(ClusterC, axis=0)

  Cluster_new = np.append([C1_mean], [C2_mean], axis =0)
  Cluster_new = np.append(Cluster_new, [C3_mean], axis = 0)
  if (1/len(Clusters))*(np.sum((Clusters - Cluster_new)**2)) < 0.1:
    print('ClusterA :',ClusterA)
    print('ClusterB :',ClusterB)
    print('ClusterC :',ClusterC)
    break
  Clusters = Cluster_new

# TRANSFORM EACH CLUSTER INTO DATAFRAMES
Cluster_A = pd.DataFrame({'Column1': ClusterA[:, 0], 'Column2': ClusterA[:, 1], 'Column3': ClusterA[:, 2], 'Column4': ClusterA[:, 3]})
Cluster_B = pd.DataFrame({'Column1': ClusterB[:, 0], 'Column2': ClusterB[:, 1], 'Column3': ClusterB[:, 2], 'Column4': ClusterB[:, 3]})
Cluster_C = pd.DataFrame({'Column1': ClusterC[:, 0], 'Column2': ClusterC[:, 1], 'Column3': ClusterC[:, 2], 'Column4': ClusterC[:, 3]})
X1 = pd.DataFrame({'Column1': X[:, 0], 'Column2': X[:, 1], 'Column3': X[:, 2], 'Column4': X[:, 3]})

# PLOTTING THE DISTRIBUTION OF THE GLOBAL DATASET AND FOR EACH CLUSTER.
fig, axs = plt.subplots(2, 2,figsize=(10,10))
X1.plot.scatter(x='Column3',y='Column4',c='DarkBlue', ax=axs[0, 0])
axs[0, 0].set_title('GLOBAL POPULATION DATASET')
Cluster_A.plot.scatter(x='Column3',
                      y='Column4',
                      c='green', ax=axs[0, 1])
axs[0, 1].set_title('CLUSTER 1')
Cluster_B.plot.scatter(x='Column3',
                      y='Column4',
                      c='red', ax=axs[1, 0])
axs[1, 0].set_title('CLUSTER 2')
Cluster_C.plot.scatter(x='Column3',
                      y='Column4',
                      c='yellow', ax=axs[1, 1])
axs[1, 1].set_title('CLUSTER 3')
for ax in axs.flat:
    ax.set(xlabel='PETHAL LENGTH', ylabel='PETHAL WIDTH')

for ax in axs.flat:
    ax.label_outer()

"""CONCLUSIONS:

> The classification of each flower is not executed correctly since only one fower belongs to cluster 2 and a few belongs to cluster 1. 

> The methodology used to select the third cluster is not the ideal one.

> The criterion chosen to define k = 3 is based on the Elbow method. This method estimates that in the point of inflection of the dataset remains the model fits best at that point. Below it is printed the ideal K by using an easy build-in function since it wasn't a requirement to plot it.
"""

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from sklearn.datasets import make_blobs
model = KMeans()
visualizer = KElbowVisualizer(model, k=(2,12))

visualizer.fit(X)

"""**DIFFERENCE BETWEEN K-NN AND K-MEANS**

> K-NN is a supervised algorithm used for classification. The labels of the data **are known** which are used to train the model and produce some learning to classify the unseen data. The data is discrete.

> K-Means is an unsupervised algorithm used for clustering. The data is **not labeled** therefore the algorithm relies on the independent features to make predictions for unseen data.

BIBIOGRAPHY:

> https://www.scikit-yb.org/en/latest/api/cluster/elbow.html

> https://www.quora.com/How-is-the-k-nearest-neighbor-algorithm-different-from-k-means-clustering
"""

