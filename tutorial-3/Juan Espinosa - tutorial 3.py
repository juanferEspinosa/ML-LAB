# -*- coding: utf-8 -*-
"""lab -tutorial 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f8I29icd2pJZZxyleBXtJEj4TjLHZIo5

# MACHINE LEARNING LAB - TUTORIAL 3
---
## Juan Fernando Espinosa
## 303158

# 1. DATA PROCESSSING

## IMPORT DATASETS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import math
import matplotlib.pyplot as plt
from google.colab import files
from google.colab import drive

drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 3/airq402.data"
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 3/parkinsons_updrs.data"
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 3/winequality-red.csv"

"""> To ease the cleaning step of the dataset I created an array with all possible incongruent fields to transform into NaN strings for future dropping. Dataframes are created."""

column_names_airq402 = ['City_1','City_2','Average Fare','Distance','Average weekly passengers','market leading airline','market share','Average fare2','Low price airline','market share2','price']
missing_values = ['-','na','Nan','nan','n/a','?']
airq402 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/LAB/tutorial 3/airq402.data',delim_whitespace=True, names=column_names_airq402, na_values = missing_values)
parkinsons_updrs = pd.read_csv('/content/drive/My Drive/Colab Notebooks/LAB/tutorial 3/parkinsons_updrs.data', sep=',', na_values = missing_values)
winequality_red = pd.read_csv('/content/drive/My Drive/Colab Notebooks/LAB/tutorial 3/winequality-red.csv', sep=';', na_values = missing_values)

"""## AIRQ402 Dataset

> Search for missing/incongruent values.
"""

airq402.dtypes
print(airq402.count())
airq402.head()

check = airq402.empty
airq402["City_1"].value_counts()
print('checking missing values:',check)
print('Sum of errors:',airq402.isnull().sum())

"""> Encode it to transform all the non-values columns into numerical values."""

dummy_airq402 = pd.get_dummies(airq402)
dummy_airq402.head()

"""> Check if there are columns that don't represent any advantage in the dataset"""

airq402_pearsonCorr = airq402.corr(method='pearson')
airq402_pearsonCorr['price']

"""**Observations:** 

* The market share2 column does not impact the price neither because the popularity of a flight in a general statement does not increase or decrease in comparison to the brand and it has the market share column which address in a better way the relationship. 
* Average Fare columns has the highest correlation. Therefore, the one which best address the relationship is going to be selected: **Average Fare**. The output measuring this Average Fare returns a good overview of the second Average Fare column. 
* Cities columns does not add value in measuring the final price even though they are crucial with a final output to find the most effective, expensive, used, cheaper, and so more examples of flights and routes. In other words, the best output of them would be "grouping by" cities category".  
* Finally, same effect with cities are experienced by airlines and market share dominant companies. Encoding the categories are helpful to know the impact of a low-price, mid-price and high-price airline but for the purpose of this exercise they turn out to be irrelevant.
"""

airq402_new=airq402.drop(columns=['Average fare2', 'market share2', 'City_1','City_2', 'Low price airline', 'market leading airline'])
airq402_new.head()

"""> It is important to scale all the results of the dataset to make more accurate studies."""

def scale(value):
    new = (value-value.min())/(value.max()-value.min())
    return new

airq402_new_scale = airq402_new.copy()

airq402_new_scale['Average weekly passengers'] = scale(airq402_new_scale['Average weekly passengers'])
airq402_new_scale['market share'] = scale(airq402_new_scale['market share'])
airq402_new_scale['Average Fare'] = scale(airq402_new_scale['Average Fare'])
airq402_new_scale['price'] = scale(airq402_new_scale['price'])
airq402_new_scale['Distance'] = scale(airq402_new_scale['Distance'])
airq402_new_scale.head()

"""> Split the dataset in **Train** and **Test** set."""

airq402_new_train = airq402_new_scale.sample(frac=0.8) 
airq402_new_test = airq402_new_scale.drop(airq402_new_train.index)

"""## Parkinsons updrs

> This dataset does not contain any categorical column, therefore, no encodification needed. In addition, no missing values found. 


> Proceeding to identify the important columns in the dataset.
"""

parkinsons_updrs.head()
check = parkinsons_updrs.empty
print('checking missing values:',check)
print('Sum of errors:',parkinsons_updrs.isnull().sum())
parkinsons_updrs.head()

Parkinson_pearsonCorr = parkinsons_updrs.corr(method='pearson')
Parkinson_pearsonCorr['total_UPDRS']

"""**Observations:** Considering a Parkinson study, the total UPDRS bear into account mental state, behavior, mood, motor examination of a patient, daily activities and thepary complications. Therefore, only the column *test_time* must be dropped.

* All the Jitter columns has pretty similar correlations. Therefore, taking into account one of them will give a good perspective of the impact of all of them. In consequence, the column **Jitter(Abs)** will remain. 

* Similar effect is experienced with Shimmer columns. From the following columns: Shimmer, Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, Shimmer:APQ11, and Shimmer:DDA, the remaining columns are going to be: **Shimmer:APQ11** and **Shimmer:APQ5.**


> Perhaps, someone could say that Sex and Age are not relevant. Especially in this case, both give insights of the illness.
"""

parkinsons_updrs_clean = parkinsons_updrs.drop(columns=['test_time','Shimmer', 'Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:DDA', 'Jitter(%)', 'Jitter:RAP', 'Jitter:PPQ5', 'Jitter:DDP'])
parkinsons_updrs_clean.head()

"""> In order to avoid large values I am going to scale all the values."""

parkinsons_updrs_clean_scale = parkinsons_updrs_clean.copy()

parkinsons_updrs_clean_scale['age'] = scale(parkinsons_updrs_clean_scale['age'])
parkinsons_updrs_clean_scale['sex'] = scale(parkinsons_updrs_clean_scale['sex'])
parkinsons_updrs_clean_scale['motor_UPDRS'] = scale(parkinsons_updrs_clean_scale['motor_UPDRS'])
parkinsons_updrs_clean_scale['Jitter(Abs)'] = scale(parkinsons_updrs_clean_scale['Jitter(Abs)'])
parkinsons_updrs_clean_scale['Shimmer:APQ5'] = scale(parkinsons_updrs_clean_scale['Shimmer:APQ5'])
parkinsons_updrs_clean_scale['Shimmer:APQ11'] = scale(parkinsons_updrs_clean_scale['Shimmer:APQ11'])
parkinsons_updrs_clean_scale['NHR'] = scale(parkinsons_updrs_clean_scale['NHR'])
parkinsons_updrs_clean_scale['HNR'] = scale(parkinsons_updrs_clean_scale['HNR'])
parkinsons_updrs_clean_scale['RPDE'] = scale(parkinsons_updrs_clean_scale['RPDE'])
parkinsons_updrs_clean_scale['DFA'] = scale(parkinsons_updrs_clean_scale['DFA'])
parkinsons_updrs_clean_scale['subject#'] = scale(parkinsons_updrs_clean_scale['subject#'])

parkinsons_updrs_clean_scale.head()

"""> Split the dataset in **Train** and **Test** set."""

parkinsons_updrs_clean_train = parkinsons_updrs_clean_scale.sample(frac=0.8) 
parkinsons_updrs_clean_test = parkinsons_updrs_clean_scale.drop(parkinsons_updrs_clean_train.index)

"""## Red Wine quality"""

winequality_red.head()

"""This dataset does not contain any categorical column, therefore, no encodification needed.  


> Proceeding to check missing values.
"""

check = winequality_red.empty
print('checking missing values:',check)
print('Sum of errors:',winequality_red.isnull().sum())
winequality_red.head()

"""No errors found in the dataset.


> Proceeding to identify the relation between categories. Considering **Quality** as the final result of all the information in the columns the relation will be hold onto quality.
"""

winequality_red_pearsonCorr = winequality_red.corr(method='pearson')
winequality_red_pearsonCorr['quality']

"""**Observations:** Residual sugar, free sulfur dioxide and pH values has a minimum impact on the final quality of a red wine. Therefore excluding from our calculus will affect in no ways the final result."""

winequality_red.drop(columns=['residual sugar', 'pH', 'free sulfur dioxide'])
winequality_red.head()

"""Scale the info in order to make it easier to run the algorithm"""

winequality_red_normalized = scale(winequality_red)
winequality_red_normalized.head()
winequality_red_normalized1=(winequality_red-winequality_red.mean())/winequality_red.std()

"""> Split the dataset in **Train** and **Test** set"""

winequality_red_normalized_train = winequality_red_normalized.sample(frac=0.8)
winequality_red_normalized_test = winequality_red_normalized.drop(winequality_red_normalized_train.index)
winequality_red_normalized_train.values
winequality_red_normalized_test.values

"""# 1.2 LINEAR REGRESSION WITH GRADIENT DESCENT

In this section to keep an order I will organize by dataset.

> As a starting point, the main code of the algorithm is presented. In sections below the code will be called by a print function to retrieve results for the different datasets.
"""

# Minimization of the gradient descent
def minimize_GD(X, y, u, num_iters, e, beta,n):
  beta_old = beta
  loss_decrease = []
  sum_RMSE = 0
  RMSE_plot = []
  for i in range(num_iters):
    y_hat = np.dot(X.T,beta_old)
    loss = sum((y_hat - y)**2)
    # Measure the values of the new betas.
    Beta_new = beta_old - u*((-2)*X@(y - X.T@beta_old))
    # Call of the function for loss calculation
    loss_calculation = function(y, X, Beta_new, beta_old)
    loss_decrease.append(loss_calculation)
    RMSE = RMSE_function(X, y, y_hat)
    sum_RMSE+=RMSE
    RMSE_plot.append(RMSE)
    beta_old = Beta_new
  return Beta_new, loss_decrease, sum_RMSE, RMSE_plot


def learn_linreg_GD(X, y, u, num_iters,e):
    X = X
    y = Y
    n = X.shape[0]
    beta = np.zeros(n)
    beta = np.reshape(beta, (len(beta),1))
    beta_hat, loss, RMSE, RMSE_plot = minimize_GD(X, y, u, num_iters, e, beta, n)
    return  beta_hat,loss, RMSE, RMSE_plot

# Function for the Loss.
def function(y, X, Beta_new, beta_old):
  a = abs(np.sum((y - X.T@beta_old)**2) - np.sum((y - X.T@Beta_new)**2))
  return a

#Function for the RMSE.
def RMSE_function(X, y, y_hat):
  a = np.sqrt(sum((y - y_hat)**2)/y.shape[0])
  return a

"""# AIRQ402 DataSet

> First, we train the algorithm by using the 80% of the data.
"""

Y = airq402_new_train['price'].values
Y = np.reshape(Y, (len(Y),1))
X = airq402_new_train.drop(['price'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Next, a graph showing us the behavior of the information are required to establish an optimum number of iterations and learning rate."""

a, b, c, c1 = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)
d, e, f, f1 = learn_linreg_GD(X.T, Y, 0.001, 1000, 0.5)
h, i, j, j1 = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
k, l, m, m1 = learn_linreg_GD(X.T, Y, 0.0001, 1000, 0.5)
n, o, p, p1 = learn_linreg_GD(X.T, Y, 0.01, 600, 0.5)
q, r, u, u1 = learn_linreg_GD(X.T, Y, 0.01, 300, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.001 and iterations: 1000.')
axs[1, 0].plot(range(100), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[1, 1].plot(range(1000), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.0001 and iterations: 1000.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.01 and iterations: 600.')
axs[2, 1].plot(range(300), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.01 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* The optimum learning rate should be minor than 0.001 otherwise the gradient  will be bigger than the previous value and at the moment of making it absolute it will increase. 

* Even though the values has been scaled they are too big that at some point it causes overflow of information.

* The more iteractions given to the algorithm, the less iterations it needs to reach convergence. It tends to take more iteractions to reach the convergence.

> Second, we test the algorithm by using the 20% remaining of data.
"""

Y = airq402_new_test['price'].values
Y = np.reshape(Y, (len(Y),1))
X = airq402_new_test.drop(['price'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Plotting the graph to see the behavior of the RMSE in regards to the number or iteractions."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 0.001, 1000, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 0.0001, 1000, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 0.01, 600, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 0.01, 300, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.001 and iterations: 1000.')
axs[1, 0].plot(range(100), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[1, 1].plot(range(1000), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.0001 and iterations: 1000.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.01 and iterations: 600.')
axs[2, 1].plot(range(300), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.01 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* Same effect happens while dealing with the learning rate. In addition, an interesting trend is possible to appreciate: a learning rate of 0.0001 reaches convergence faster than a learning rate of 0.001. 

* Addressing the RMSE, the more iterations the algorithm runs it show us the good performance and minimization of the error. As it is possible to see in figure 2 with 1000 iteractions, from 300 iterations and on the algorithm has reached convergence. Therefore, it is an optimal number of iterations.

# Parkinsons Updrs

> First, we train the algorithm by using the 80% of the data.
"""

Y = parkinsons_updrs_clean_train['total_UPDRS'].values
Y = np.reshape(Y, (len(Y),1))
X = parkinsons_updrs_clean_train.drop(['total_UPDRS'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Next, a graph showing us the behavior of the information are required to establish an optimum number of iterations and learning rate."""

a, b, c, c1 = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)
d, e, f, f1 = learn_linreg_GD(X.T, Y, 0.001, 1000, 0.5)
h, i, j, j1 = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
k, l, m, m1 = learn_linreg_GD(X.T, Y, 0.0001, 1000, 0.5)
n, o, p, p1 = learn_linreg_GD(X.T, Y, 0.00001, 600, 0.5)
q, r, u, u1 = learn_linreg_GD(X.T, Y, 0.00001, 300, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.001 and iterations: 1000.')
axs[1, 0].plot(range(100), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[1, 1].plot(range(1000), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.0001 and iterations: 1000.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.00001 and iterations: 600.')
axs[2, 1].plot(range(300), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* The optimum learning rate should be minor than 0.0001 otherwise the gradient  will be bigger than the previous value and at the moment of making it absolute it will increase as it is possible to appreciate in the first 4 graphs. 

* Even though the values has been scaled they are too big that at some point it causes overflow of information.

> Second, we test the algorithm by using the 20% remaining of data.
"""

Y = parkinsons_updrs_clean_test['total_UPDRS'].values
Y = np.reshape(Y, (len(Y),1))
X = parkinsons_updrs_clean_test.drop(['total_UPDRS'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Plotting the graph to see the behavior of the RMSE in regards to the number or iteractions."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 0.001, 1000, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 0.0001, 1000, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 0.00001, 600, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 0.00001, 300, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.001 and iterations: 1000.')
axs[1, 0].plot(range(100), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[1, 1].plot(range(1000), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.0001 and iterations: 1000.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.00001 and iterations: 600.')
axs[2, 1].plot(range(300), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* Graph 3 and 5 shows a different behavior as the previous seen. The increasing number of iteraction has a minimum effect in reaching convergence and also the variation in the learning rate is not having an impact in the velocity of the gradient. 

* Learning rates less than < 0.0001 returns a gradient bigger than the Betas values making the curve to increase rather than to decrease.

# Red Wine Quality

> First step: Implementation of the linear regression
"""

Y = winequality_red_normalized_train['quality'].values
Y = np.reshape(Y, (len(Y),1))
Ytest = winequality_red_normalized_test['quality'].values
Ytest = np.reshape(Ytest, (len(Ytest),1))
X = winequality_red_normalized_train.drop(['quality'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)



def minimize_GD(X, y, u, num_iters, e, beta,n):
  beta_old = beta
  loss_decrease = []
  sum_RMSE = 0
  for i in range(num_iters):
    y_hat = np.dot(X.T,beta_old)
    loss = sum((y_hat - y)**2)
    Beta_new = beta_old - u*((-2)*X@(y - X.T@beta_old))
    loss_calculation = function(y, X, Beta_new, beta_old)
    loss_decrease.append(loss_calculation)
    beta_old = Beta_new
    #RMSE = RMSE_function(Ytest, y_hat, X)
  return Beta_new, loss_decrease
    
def learn_linreg_GD(X, y, u, num_iters,e):
    X = X
    y = Y
    n = X.shape[0]
    beta = np.zeros(n)
    beta = np.reshape(beta, (len(beta),1))
    beta_hat, loss = minimize_GD(X, y, u, num_iters, e, beta, n)
    return  beta_hat,loss


def function(y, X, Beta_new, beta_old):
  a = abs(np.sum((y - X.T@beta_old)**2) - np.sum((y - X.T@Beta_new)**2))
  return a

a, b = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)

print('Betas', a,'\n', 'Loss', Loss)

"""> In order to check how many iteractions, learning rates, and suitable learning rate the following plots are needed."""

a, b = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)
c, d = learn_linreg_GD(X.T, Y, 0.001, 1000, 0.5)
e, f = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
g, h = learn_linreg_GD(X.T, Y, 0.0001, 1000, 0.5)
i, j = learn_linreg_GD(X.T, Y, 0.0001, 600, 0.5)
k, l = learn_linreg_GD(X.T, Y, 0.0001, 300, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), d, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.001 and iterations: 1000.')
axs[1, 0].plot(range(100), f, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[1, 1].plot(range(1000), h, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.0001 and iterations: 1000.')
axs[2, 0].plot(range(600), j, 'tab:red')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), l, 'tab:red')
axs[2, 1].set_title('Learning rate: 0.0001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* The optimum learning rate should be minor than 0.001 otherwise the gradient  will be bigger than the previous value and at the moment of make it absolute it will increase. 

* Even though the values has been scaled they are too big that at some point it causes overflow of information.

* The more iteractions the algorithm takes, it reaches the optimum value faster. As we can see in plots 3-5-6 it tends to take more iteractions to reach the convergence.

> In order to add the RMSE, The code above is duplicated and the **test set** will be used to find out th performance of the model.
"""

Y = winequality_red_normalized_test['quality'].values
Y = np.reshape(Y, (len(Y),1))
X = winequality_red_normalized_test.drop(['quality'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)


def minimize_GD(X, y, u, num_iters, e, beta,n):
  beta_old = beta
  loss_decrease = []
  sum_RMSE = 0
  RMSE_array = []
  for i in range(num_iters):
    y_hat = np.dot(X.T,beta_old)
    loss = sum((y_hat - y)**2)
    Beta_new = beta_old - u*((-2)*X@(y - X.T@beta_old))
    loss_calculation = function(y, X, Beta_new, beta_old)
    loss_decrease.append(loss_calculation)
    beta_old = Beta_new
    RMSE = RMSE_function(X, Ytest, y_hat)
    sum_RMSE+=RMSE
    RMSE_array.append(RMSE)
  return Beta_new, loss_decrease, sum_RMSE, RMSE_array
    
def learn_linreg_GD(X, y, u, num_iters,e):
    X = X
    y = Y
    n = X.shape[0]
    beta = np.zeros(n)
    beta = np.reshape(beta, (len(beta),1))
    beta_hat, loss, RMSE, total_RMSE = minimize_GD(X, y, u, num_iters, e, beta, n)
    return  beta_hat,loss, RMSE, total_RMSE


def function(y, X, Beta_new, beta_old):
  a = abs(np.sum((y - X.T@beta_old)**2) - np.sum((y - X.T@Beta_new)**2))
  return a

def RMSE_function(Xtest, Ytest, y_hat):
  a = np.sqrt(sum((Ytest - y_hat)**2)/Ytest.shape[0])
  return a

a, b, c, d = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)

print('Betas', a,'\n', 'Loss', Loss,'\n' 'RMSE', c)

"""> Plotting the graph to see the behavior of the RMSE in regards to the number or iteractions."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 0.001, 100, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 0.001, 1000, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 0.0001, 100, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 0.0001, 1000, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 0.00001, 600, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 0.00001, 300, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.001 and iterations: 1000.')
axs[1, 0].plot(range(100), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[1, 1].plot(range(1000), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.0001 and iterations: 1000.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.00001 and iterations: 600.')
axs[2, 1].plot(range(300), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* A lower learning rate works better as we can appreciate in figure 1-3.  

* Moreover, in the iteraction 600 (more or less) the RMSE is minimized until reach convergence and keep going to the infinite.

# 1.2 **PART B** STEP LENGTH FOR GRADIENT DESCENT

In order to keep organization the information is going to be presented in organization according to the algorithm.

# Steplength Backtracking

> As a starting point the main code of the algorithm is presented from which future calculations in each dataset are going to be executed.
"""

# DEF general function F(x) for multivariate Linear Regression
def main_function(X, y, beta_old):
  main_function = np.dot(((y - X.T@beta_old).T),(y - X.T@beta_old))
  return main_function

# DEF of the derivative of the function
def derivative(X, y, beta_old):
  derivative = ((-2)*X@(y - X.T@beta_old))
  return derivative

def main_function_new(X, y, u, beta_old):
  new_function_X = main_function(X, y,beta_old - u*derivative(X, y, beta_old))
  return new_function_X


def stepsize_backtracking(X, y, beta_old, a, b):
  u = 1.
  left = main_function_new(X, y, u, beta_old)
  right = main_function(X, y, beta_old) - a*u*((derivative(X, y, beta_old)).T@(derivative(X, y, beta_old)))
  while (left) > (right):
    u = b*u
    left = main_function_new(X, y, u, beta_old)
    right = main_function(X, y, beta_old) - a*u*((derivative(X, y, beta_old)).T@(derivative(X, y, beta_old)))
  return u

# Minimization of the gradient descent
def minimize_GD(X, y, u, num_iters, beta, n, a, b):
  beta_old = beta
  loss_decrease = []
  sum_RMSE = 0
  RMSE_plot = []
  u_array = []
  for i in range(num_iters):
    y_hat = np.dot(X.T,beta_old)
    loss = sum((y_hat - y)**2)
    # Stepsize Backtracking
    u = stepsize_backtracking(X, y, beta_old, a, b)

    # Measure the values of the new betas.
    Beta_new = beta_old - u*derivative(X, y, beta_old)
    # Call of the function for loss calculation
    loss_calculation = function(y, X, Beta_new, beta_old)
    loss_decrease.append(loss_calculation)
    RMSE = RMSE_function(X, y, y_hat)
    sum_RMSE+=RMSE
    RMSE_plot.append(RMSE)
    u_array.append(u)
    beta_old = Beta_new
  return Beta_new, loss_decrease, sum_RMSE, RMSE_plot


def learn_linreg_GD(X, y, num_iters, a, b):
    X = X
    y = Y
    n = X.shape[0]
    beta = np.zeros(n)
    beta = np.reshape(beta, (len(beta),1))
    beta_hat, loss, RMSE, RMSE_plot = minimize_GD(X, y, u, num_iters, beta, n, a, b)
    return  beta_hat,loss, RMSE, RMSE_plot

# Function for the Loss.
def function(y, X, Beta_new, beta_old):
  a = abs(np.sum((y - X.T@beta_old)**2) - np.sum((y - X.T@Beta_new)**2))
  return a

#Function for the RMSE.
def RMSE_function(X, y, y_hat):
  a = np.sqrt(sum((y - y_hat)**2)/y.shape[0])
  return a

"""## AIRQ402 Dataset

First, we train the algorithm in the training dataset.
"""

Y = airq402_new_train['price'].values
Y = np.reshape(Y, (len(Y),1))
X = airq402_new_train.drop(['price'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Plot the information in regards to the Loss"""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.01, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 500, 0.01, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 300, 0.00001, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.001, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 400, 0.00001, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.01 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.01 and iterations: 1000.')
axs[1, 0].plot(range(500), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.01 and iterations: 500.')
axs[1, 1].plot(range(300), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.00001 and iterations: 300.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.001 and iterations: 600.')
axs[2, 1].plot(range(400), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 400.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='Loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""> Test the algorithm in the test set to see how accurate the algorithm is."""

Y = airq402_new_test['price'].values
Y = np.reshape(Y, (len(Y),1))
X = airq402_new_test.drop(['price'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> PLot the behavior of the RMSE in regards to the iteractions."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.01, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 500, 0.01, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 300, 0.00001, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.001, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 400, 0.00001, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.01 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.01 and iterations: 1000.')
axs[1, 0].plot(range(500), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.01 and iterations: 500.')
axs[1, 1].plot(range(300), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.00001 and iterations: 300.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.001 and iterations: 600.')
axs[2, 1].plot(range(400), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 400.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* As an easy observation, the event that we have seen the most happen when the learning rate is not big. It makes bigger steps to reach convergence faster.

## Parkinsons UPDRS dataset

> First, we train the algorithm in the training dataset.
"""

Y = parkinsons_updrs_clean_train['total_UPDRS'].values
Y = np.reshape(Y, (len(Y),1))
X = parkinsons_updrs_clean_train.drop(['total_UPDRS'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> We plot a visualization of the behavior of the data in the set"""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.01, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 500, 0.01, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 300, 0.00001, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.001, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 400, 0.00001, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.01 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.01 and iterations: 1000.')
axs[1, 0].plot(range(500), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.01 and iterations: 500.')
axs[1, 1].plot(range(300), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.00001 and iterations: 300.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.001 and iterations: 600.')
axs[2, 1].plot(range(400), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 400.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='Loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* It takes more or less 15 iterations to reach convergence with a 0.1 learning rate. Independently of the learning rate, the loss decrease with high speed no matter the number of iterations.

> Check the accuracy of the algorithm in the test set
"""

Y = parkinsons_updrs_clean_test['total_UPDRS'].values
Y = np.reshape(Y, (len(Y),1))
X = parkinsons_updrs_clean_test.drop(['total_UPDRS'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Plot the RMSE values in regards to the number of iterations"""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.01, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 500, 0.01, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 300, 0.00001, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.001, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 400, 0.00001, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.01 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.01 and iterations: 1000.')
axs[1, 0].plot(range(500), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.01 and iterations: 500.')
axs[1, 1].plot(range(300), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.00001 and iterations: 300.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.001 and iterations: 600.')
axs[2, 1].plot(range(400), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 400.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""In comparison with the graphs above, we can see how much the iterations increase in a graph with high learning rate (1). The less number of iterations and a lower value of learning rate fits a good curvature to reach convergence (6).

## Red Wine Quality

> First, we train the algorithm in the training dataset.
"""

Y = winequality_red_normalized_train['quality'].values
Y = np.reshape(Y, (len(Y),1))
Ytest = winequality_red_normalized_test['quality'].values
Ytest = np.reshape(Ytest, (len(Ytest),1))
X = winequality_red_normalized_train.drop(['quality'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Plot of the results in regards to the Loss and iterations"""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.01, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 500, 0.01, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 300, 0.00001, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.001, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 400, 0.00001, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.01 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.01 and iterations: 1000.')
axs[1, 0].plot(range(500), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.01 and iterations: 500.')
axs[1, 1].plot(range(300), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.00001 and iterations: 300.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.001 and iterations: 600.')
axs[2, 1].plot(range(400), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 400.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='Loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""> Test the effectiveness of the model in the test set."""

Y = winequality_red_normalized_test['quality'].values
Y = np.reshape(Y, (len(Y),1))
X = winequality_red_normalized_test.drop(['quality'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""> Plot the information in the test set."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.01, 0.5)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.01, 0.5)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 500, 0.01, 0.5)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 300, 0.00001, 0.5)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.001, 0.5)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 400, 0.00001, 0.5)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.01 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.01 and iterations: 1000.')
axs[1, 0].plot(range(500), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.01 and iterations: 500.')
axs[1, 1].plot(range(300), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.00001 and iterations: 300.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.001 and iterations: 600.')
axs[2, 1].plot(range(400), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.00001 and iterations: 400.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""As we can see the curvature increases at the same pace the learning rate increases. In addition, the number of iterations is key: it reaches convergence with a minimum number of iterations. 100 and thousand iterations are too high to reach convergence.

# Bold Driver Step Size

## The algorithm is presented
"""

# DEF general function F(x) for multivariate Linear Regression
def main_function(X, y, beta_old):
  main_function = (((y - X.T@beta_old).T)**2)
  return main_function

# DEF of the derivative of the function
def derivative(X, y, beta_old):
  derivative = ((-2)*X@(y - X.T@beta_old))
  return derivative

# Main function used to find 
def main_function_new(X, y, u, beta_old):
  new_function_X = main_function(X, y,beta_old - u*derivative(X, y, beta_old))
  return new_function_X

def conditional_function(X, y, beta_old, u):
  conditional_function = main_function(X, y, beta_old)
  return conditional_function


def steplength_bolddriver(X, y, a, b, u, beta_old, u_old, u_plus, u_minus):
  u = u_old * u_plus
  while np.all(main_function(X, y, beta_old)) <= np.all(main_function_new(X, y, u, beta_old)):
    u = u*u_minus
    return u

# Minimization of the gradient descent
def minimize_GD(X, y, u, num_iters, beta, n, u_old, u_plus, u_minus):
  beta_old = beta
  loss_decrease = []
  sum_RMSE = 0
  RMSE_plot = []
  u_array = []
  for i in range(num_iters):
    y_hat = np.dot(X.T,beta_old)
    loss = sum((y_hat - y)**2)
    # Stepsize Backtracking
    u = steplength_bolddriver(X, y, a, b, u, beta_old, u_old, u_plus, u_minus)

    # Measure the values of the new betas.
    Beta_new = beta_old - u*derivative(X, y, beta_old)
    # Call of the function for loss calculation
    loss_calculation = function(y, X, Beta_new, beta_old)
    loss_decrease.append(loss_calculation)
    RMSE = RMSE_function(X, y, y_hat)
    sum_RMSE+=RMSE
    RMSE_plot.append(RMSE)
    u_array.append(u)
    beta_old = Beta_new
  return Beta_new, loss_decrease, sum_RMSE, RMSE_plot


def learn_linreg_GD(X, y, num_iters, u_old, u_plus, u_minus):
    X = X
    y = Y
    n = X.shape[0]
    beta = np.zeros(n)
    beta = np.reshape(beta, (len(beta),1))
    beta_hat, loss, RMSE, RMSE_plot = minimize_GD(X, y, u, num_iters, beta, n, u_old, u_plus, u_minus)
    return  beta_hat,loss, RMSE, RMSE_plot

# Function for the Loss.
def function(y, X, Beta_new, beta_old):
  a = abs(np.sum((y - X.T@beta_old)**2) - np.sum((y - X.T@Beta_new)**2))
  return a

#Function for the RMSE.
def RMSE_function(X, y, y_hat):
  a = np.sqrt(sum((y - y_hat)**2)/y.shape[0])
  return a

"""## AIRQ402 dataset

First, the results of the algorithm in the present dataset are shown.
"""

Y = airq402_new_train['price'].values
Y = np.reshape(Y, (len(Y),1))
X = airq402_new_train.drop(['price'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 100, 0.001, 1.5, 0.2)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""Plotting the relation between the loss function and the iterations to see the impact and the curve until convergence."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.0001, 1.5, 0.2)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.00001, 1.5, 0.2)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 100, 0.00001, 1.5, 0.2)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 1000, 0.000001, 1.5, 0.2)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.0001, 1.5, 0.2)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 300, 0.001, 1.5, 0.2)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.00001 and iterations: 1000.')
axs[1, 0].plot(range(100), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.00001 and iterations: 100.')
axs[1, 1].plot(range(1000), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.000001 and iterations: 1000.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='Loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""> Bloque con sangría

**Observations:**
* A lower learning rate works better as we can appreciate in figure 5-6. Moreover, the less learning rate, the more it takes for the algorithm to converge. 

* The lower the learning rate, the RMSE tends to decrease faster than in the other graphs. In formal words, increase its effectiveness.

> Check the efectiveness of the algorithm in the test set.
"""

Y = airq402_new_test['price'].values
Y = np.reshape(Y, (len(Y),1))
X = airq402_new_test.drop(['price'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 100, 0.001, 1.5, 0.2)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""Plotting the relation between the RMSE and the iterations to see the impact and the curve until convergence."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.0001, 1.5, 0.2)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.00001, 1.5, 0.2)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 100, 0.00001, 1.5, 0.2)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 1000, 0.000001, 1.5, 0.2)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.0001, 1.5, 0.2)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 300, 0.001, 1.5, 0.2)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.00001 and iterations: 1000.')
axs[1, 0].plot(range(100), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.00001 and iterations: 100.')
axs[1, 1].plot(range(1000), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.000001 and iterations: 1000.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* In relation to the RMSE, the convergence is reached faster when a low learning rate and low number of iteractions are given. The algorithm makes big steps at the beginning to go faster. On the other hand lower learning rate gives small chances to big steps. 
*In the graphs 4 and 5 a line is plotted because of the lower number of learning rate.

## Parkinsons UPDRS dataset

First, the results of the algorithm in the present dataset are shown.
"""

Y = parkinsons_updrs_clean_train['total_UPDRS'].values
Y = np.reshape(Y, (len(Y),1))
X = parkinsons_updrs_clean_train.drop(['total_UPDRS'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 100, 0.001, 1.5, 0.2)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""Plotting the relation between the loss function and the iterations to see the impact and the curve until convergence."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.0001, 1.5, 0.2)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.00001, 1.5, 0.2)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 100, 0.00001, 1.5, 0.2)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 1000, 0.000001, 1.5, 0.2)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.0001, 1.5, 0.2)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 300, 0.001, 1.5, 0.2)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.00001 and iterations: 1000.')
axs[1, 0].plot(range(100), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.00001 and iterations: 100.')
axs[1, 1].plot(range(1000), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.000001 and iterations: 1000.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='Loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* A lower learning rate allows us to see how the curve of the loss values go down until convergence. Unfortunately, selecting a high value of learning rate as shown in figure 6 makes the algorithm to surpace the optimum.

> Check the efectiveness of the algorithm in the test set.
"""

Y = parkinsons_updrs_clean_test['total_UPDRS'].values
Y = np.reshape(Y, (len(Y),1))
X = parkinsons_updrs_clean_test.drop(['total_UPDRS'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 100, 0.001, 1.5, 0.2)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""Plotting the relation between the RMSE and the iterations to see the impact and the curve until convergence."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.0001, 1.5, 0.2)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.00001, 1.5, 0.2)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 100, 0.00001, 1.5, 0.2)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 1000, 0.000001, 1.5, 0.2)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.0001, 1.5, 0.2)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 300, 0.001, 1.5, 0.2)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.00001 and iterations: 1000.')
axs[1, 0].plot(range(100), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.00001 and iterations: 100.')
axs[1, 1].plot(range(1000), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.000001 and iterations: 1000.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* Minimum difference can be appreciated in the relationship between RMSE and iteractions. In all the graphs the behavior is similar.

## Red Wine Quality

First, we measure the algorithm in the training set.
"""

Y = winequality_red_normalized_train['quality'].values
Y = np.reshape(Y, (len(Y),1))
Ytest = winequality_red_normalized_test['quality'].values
Ytest = np.reshape(Ytest, (len(Ytest),1))
X = winequality_red_normalized_train.drop(['quality'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 100, 0.001, 1.5, 0.2)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""Plotting the relation between the loss function and the iterations to see the impact and the curve until convergence."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.0001, 1.5, 0.2)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.00001, 1.5, 0.2)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 100, 0.00001, 1.5, 0.2)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 1000, 0.000001, 1.5, 0.2)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.0001, 1.5, 0.2)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 300, 0.001, 1.5, 0.2)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), b)
axs[0, 0].set_title('Learning rate: 0.0001 and iterations: 100.')
axs[0, 1].plot(range(1000), e, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.00001 and iterations: 1000.')
axs[1, 0].plot(range(100), i, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.00001 and iterations: 100.')
axs[1, 1].plot(range(1000), l, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.000001 and iterations: 1000.')
axs[2, 0].plot(range(600), o, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), r, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='Loss')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* A bigger learning rate allows us to see how the curve of the loss goes donw faster than the others. Figure 6. Nonetheless, lower learning rate predict with more accuracy.
* The smaller the learning rate the more pronounced curvature it will have.

> Check the efectiveness of the algorithm in the test set.
"""

Y = winequality_red_normalized_test['quality'].values
Y = np.reshape(Y, (len(Y),1))
X = winequality_red_normalized_test.drop(['quality'], axis=1).values
column_one = np.ones((X.shape[0],1))
X = np.concatenate((column_one, X), axis = 1)

Betas, Loss, RMSE, RMSE_plot = learn_linreg_GD(X.T, Y, 100, 0.001, 1.5, 0.2)
print('Betas', Betas,'\n', 'Loss', Loss,'\n', 'RMSE', RMSE)

"""Plotting the relation between the RMSE and the iterations to see the impact and the curve until convergence."""

a, b, c, f1 = learn_linreg_GD(X.T, Y, 100, 0.0001, 1.5, 0.2)
d, e, f, f2 = learn_linreg_GD(X.T, Y, 1000, 0.00001, 1.5, 0.2)
h, i, j, f3 = learn_linreg_GD(X.T, Y, 100, 0.00001, 1.5, 0.2)
k, l, m, f4 = learn_linreg_GD(X.T, Y, 1000, 0.000001, 1.5, 0.2)
n, o, p, f5 = learn_linreg_GD(X.T, Y, 600, 0.0001, 1.5, 0.2)
q, r, u, f6 = learn_linreg_GD(X.T, Y, 300, 0.001, 1.5, 0.2)
fig, axs = plt.subplots(3, 2,figsize=(15,15))
axs[0, 0].plot(range(100), f1)
axs[0, 0].set_title('Learning rate: 0.001 and iterations: 100.')
axs[0, 1].plot(range(1000), f2, 'tab:orange')
axs[0, 1].set_title('Learning rate: 0.00001 and iterations: 1000.')
axs[1, 0].plot(range(100), f3, 'tab:green')
axs[1, 0].set_title('Learning rate: 0.00001 and iterations: 100.')
axs[1, 1].plot(range(1000), f4, 'tab:red')
axs[1, 1].set_title('Learning rate: 0.000001 and iterations: 1000.')
axs[2, 0].plot(range(600), f5, 'tab:blue')
axs[2, 0].set_title('Learning rate: 0.0001 and iterations: 600.')
axs[2, 1].plot(range(300), f6, 'tab:gray')
axs[2, 1].set_title('Learning rate: 0.001 and iterations: 300.')

for ax in axs.flat:
    ax.set(xlabel='iteractions', ylabel='RMSE')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

"""**Observations:**
* Lower learning rate, the more calculations are required and the RMSE diminishes efectively. A bigger learning rate experiments otherwise.

# COMPARISON BETWEEN MODELS

* Bold Driver algorithm reaches convergence at a faster pace than the rest because since the condition determines if the step size must grow or decrease. If the $F_0(B)$ decreases, the learning rate could do bigger steps. Same happens otherwise. 

 * Backtracking algo reduces the number of iterations until convergence because each time it is fitting a perfect learning rate which is optimum to reach convergence. Nonetheless, in order to find that ideal learning rate it takes a number of iteractions that the previous model don't experience.

 * The basic multivariate linear regression model works well but it is not effective when talking about big datasets. It is time-consuming and expensive. Therefore, it is a good call to choose adaptative learning rate algorithms.
"""