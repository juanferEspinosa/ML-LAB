# -*- coding: utf-8 -*-
"""Lab - tutorial II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dcl6WNNMyz1wsP3fDgII1O0nO-YX5TC1

# MACHINE LEARNING LAB - TUTORIAL 2
---
## Juan Fernando Espinosa
## 303158

# 1. Pandas: Data Exploration

## Import of the dataset: *import-85.names*
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/My Drive/Colab Notebooks/LAB/tutorial 2/imports-85.data"

column_names = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']
missing_values = ['-','na','Nan','nan','n/a','?']
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/LAB/tutorial 2/imports-85.data', names=column_names, na_values = missing_values)
data = pd.DataFrame(data)

data.head()

"""## Fix missing or incongruent values in the dataset."""

check = data.empty
print('checking missing values:',check)
print('Sum of errors:',data.isnull().sum())

"""## Replace those empty values with the mean for each column."""

data['normalized-losses'] = data['normalized-losses'].fillna((data['normalized-losses'].mean()))
data['bore'] = data['bore'].fillna((data['bore'].mean()))
data['stroke'] = data['stroke'].fillna((data['stroke'].mean()))
data['horsepower'] = data['horsepower'].fillna((data['horsepower'].mean()))
data['peak-rpm'] = data['peak-rpm'].fillna((data['peak-rpm'].mean()))
data['price'] = data['price'].fillna((data['price'].mean()))

"""## Since num-of-doors are integers, it is necessary to fill those empty fields with real information. By finding relevant information about the cars we found that most sedans have 4 doors."""

print(data[data["num-of-doors"].isnull()])
print(data.iloc[[27,63], [2,3,4,5,6]])

data['num-of-doors'] = data['num-of-doors'].fillna(('four'))

print(data.iloc[[27,63], [2,3,4,5,6]])

"""# 1. 1\. Find the mean, median and standard deviation for each NUMERIC Column"""

numeric_data = data.select_dtypes(include=np.number)

numeric_data.head()

"""## Mean of all the columns"""

numeric_data.mean(axis=0)

"""## Median of all the columns"""

numeric_data.median(axis=0)

"""## Standard deviation of all the columns"""

numeric_data.std(axis=0)

"""# 1. 2\. Group data by the field ’make’"""

makeField_data = data.groupby(['make'])
makeField_data.first()

"""## Find the average price , average highway-mpg and average city-mpg for each make."""

makeField_data['price', 'highway-mpg', 'city-mpg'].mean()

"""## Use a seaborn pairplot to visualize all int64 data types. Explain the plot what information can we take out of it"""

int64 = data[['make','symboling', 'curb-weight','engine-size','city-mpg','highway-mpg']] 
int64.head()

import seaborn as sns; sns.set(style="ticks", color_codes=True)

g = sns.pairplot(int64, hue='make')

"""As we can appreciate in the charts the diagonal gives us the distribution of each variable while the scatter plots give us the relationship between 2 variables. 

## Observations:



> **Symboling:** As it is possible to appreciate, cars located in the neutral "risky" position tends to differ in the second variables showed, demonstrating no correlation between a risky car and the specifications of it. 

> **Curb-Weight**: There is no correlation between curb_weight and symboling. The opposite happens with *engine-size*: there is a linear distribution to the right: the more curb-weight a car has the more engine-size it will have. Finally, *city-mpg* and *highway-mpg* have a negative tendency: the more curb-weight, the less city mpg and highway-mpg a car could go.


> **engine-size:** It keeps a linear distribution with the variables: a directly proportional to its curb-weight. The engine-size affects the total curb-weight. On the contrary, the more engine-size it means a better fuel optimization which decreases the city and highway mpg.

> **City and highway MPG:** both variables has a directly proportionality. If one decreases the other as well because of the same engine-size and vehicle characteristics. Moreover as mentioned before, the higher curb-weight and engine-size the less MPG the car will have. 

## Conclusions:

+ A risky or non-risky car not necessarily accomplish the best characteristics.
+ The more engine-size/curb-weight the more money a person will save in fuel.

---

## Similar to the first exercise use city-mpg as your dependant variable and engine-size as the independent value. Fit a line, use scatterplot for the data points and plot the line you predicted on top
"""

Linear_regression = data[['engine-size', 'city-mpg']]
Linear_regression.head()
mean_horsepower = Linear_regression.mean(axis = 0)
mean1 = mean_horsepower['engine-size']
mean2 = mean_horsepower['city-mpg']

numerator = (Linear_regression['engine-size'] - mean1)*(Linear_regression['city-mpg'] - mean2)
totalNum = 0
for i in numerator:
  totalNum += i

denominator = (Linear_regression['engine-size'] - mean1)**2
totalDen = 0
for i in denominator:
    totalDen += i

# Calculus of the Betas
beta_1 = totalNum / totalDen
beta_0 = mean2 - (beta_1*(mean1))
print('Beta0:', beta_0)
print('Beta1:', beta_1)

# Prediction of y for all datapoints in X.
y_prediction = []
p1 = beta_1*Linear_regression['engine-size'] + beta_0
y_prediction = p1
print("y_prediction:", y_prediction)


#Plotting the graph considering the Betas found and the predictions.
fig_size = plt.rcParams["figure.figsize"]
fig_size[0] = 15
fig_size[1] = 6
plt.rcParams["figure.figsize"] = fig_size

plt.scatter(Linear_regression['engine-size'], Linear_regression['city-mpg'], color='green', alpha=0.5)
plt.title('Linear Regression')
plt.xlabel('engine-size')
plt.ylabel('city mpg')
plt.plot(Linear_regression['engine-size'],y_prediction, c ='blue')
plt.show()

"""**note:** The graph has been made considering the variables because it does not make sense to create a plot segmented by "make" group. 

**Observations:** It is not a good predicttion because the fit does not capture the esence of the dataset and could infere in **underfitting**. Underfitting means a lack in capturing the underlying structure of the data. Therefore, in this particular example the line overfit the nearest datapoints and does not cover several fa-away positioned datapoints. It is not a good prediction.

# 2. Linear Regression via Normal Equations

## Reuse dataset from Excercise 1. Load it as Xdata
"""

x_features = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']
Xdata = data[x_features]
Ydata = data.price

"""## Choose those columns, which can help you in prediction i.e. contain some useful information. You can drop irrelevant columns. Give reason for choosing or dropping any column.

First a good measure of columns which are going to have an impact on the prediction are the ones with correlation with the dependant variables because its influence on it.
"""

pearsonCorr = data.corr(method='pearson')
pearsonCorr['price']

"""According to the relation between the price column and the independent columns (Xdata) it is optimal to consider all of them which has an influence on the price of the vehicle. Therefore, all columns with correlation near to 0 are going to be dropped. 

---
"""

Xdata = data.drop(["symboling", "normalized-losses", "height", "stroke", "compression-ratio", "peak-rpm", "price"], axis=1) 
#Xdata = pd.DataFrame(data, columns=['wheel-base',	'length',	'width',	'curb-weight',	'engine-size',	'bore',	'horsepower',	'city-mpg',	'highway-mpg'])
Xdata = Xdata.select_dtypes(include=np.number)
Xdata.insert(0, 'Column of 1', 1)

"""Step for adding a column of ones to the Xdata dataframe."""

Xdata = pd.DataFrame(Xdata)

Ydata = pd.DataFrame(Ydata)

Xdata_array = Xdata.rename_axis('datas').values
Ydata_array = Ydata.rename_axis('datas1').values
print(Xdata_array)
Ydata_array.round()
Xdata_array.round()

"""## Split your dataset Xdata, Ydata into Xtrain, Ytrain and Xtest, Ytest i.e. you can randomly assign 80% of the data to a Xtrain, Ytrain set and remaining 20% to a Xtest, ytest set."""

Xtrain = Xdata.sample(frac=0.8) 
Ytrain = Ydata.sample(frac=0.8)
Xtest = Xdata.drop(Xtrain.index)
Ytest = Ydata.drop(Ytrain.index) 

Xtest.to_numpy().round()
Ytest.to_numpy().round()

# It will help in the calculus of the y predictions.
Xtest = np.array(Xtest)
Ytest = np.array(Ytest)

"""## Implement learn-linreg-NormEq algorithm and learn a parameter vector β using Xtrain set. You have to learn a model to predict sales price of cars i.e. , ytest."""

X = Xtrain.T
A = np.dot(X, Xtrain)
b = np.dot(X, Ytrain)
Betas_LSE = np.linalg.solve(A, b)
print('Betas',Betas_LSE)

"""## Line 6, in learn-linreg-NormEq uses SOLVE-SLE. You have to replace SOLVE-SLE

+ Gaussian Elimination
"""

def Gaussian_Elimination(A, b):
 
    n =  len(A)
    # Find the maximum value in the first column and diagonal of the matrix.
    for i in range(len(A)-1):
        # Swaping columns to put the maximum value as the first row 
        max_row = abs(A[i:,i]).argmax() + i
        if max_row != i:
            A[[i,max_row]] = A[[max_row, i]]
            b[[i,max_row]] = b[[max_row, i]]
        for j in range(i+1, len(A)):
            ratio = A[j][i]/A[i][i]
            # Select the values other than the selected one for making those zeros.
            A[j][i] = ratio
            for k in range(i + 1, len(A)):
                A[j][k] = A[j][k] - ratio*A[j][k]
            
            # Updating items for each row.

            b[j] = b[j] - ratio*b[j]

# Return the final values.
    x = np.zeros(len(A))
    j = len(A)-1
    x[j] = b[j]/A[j,j]
    while j >= 0:
        x[j] = (b[j] - np.dot(A[j,j+1:],x[j+1:]))/A[j,j]
        j = j-1
    return x

Xtrain.shape
X = Xtrain.T
X.shape
A = np.dot(X, Xtrain)
b = np.dot(X, Ytrain)
Betas_Gaussian_Elimination = Gaussian_Elimination(A, b)
print('Betas',Betas_Gaussian_Elimination)

"""+ QR Decomposition"""

def QR(A):
  u =[]
  e = []
  u.append(A[:,0])
  e.append(u[0]/ np.sqrt(np.sum(u[0]**2)))

  for i in range(1,len(A[0])):
    current_a = A[:,i]
    current_u = current_a 
    for j in range(0,len(u)):
      current_u -= ((current_a @ e[j])*e[j])
    u.append(current_u)
    e.append(u[i]/ np.sqrt(np.sum(u[i]**2)))
  return np.array(u), np.array(e)

A1 = A.T
A2 = np.append(A2, b, axis=1)
q = e.T
r = np.dot(q,A2)
print('Q', q)
print('R', r)

"""As it is possible to appreciate in the results of the QR decomposition, the values in the lower triangle of the matrix does not equal 0 which is one the premises to find the values of the betas. Therefore, it is not 100% accurate to use this process.

## Perform prediction y on test dataset i.e. Xtest using the set of parameters learned
"""

y_prediction = np.matmul(Xtest, Betas_LSE)
print('Betas LSE', y_prediction)
y_prediction.shape

y_prediction_gaussian_elimination = np.matmul(Xtest, Betas_Gaussian_Elimination)
print('Betas Gaussian Elimination', y_prediction_gaussian_elimination)

"""As we have different random distribution, the results are different for both predictions.

## Final step is to find how close these two models are to the original values.

### Plot the residual
"""

Residual_LSE = []
for i in range(0, len(y_prediction)):
  a = abs(Ytest[i] - y_prediction[i])
  Residual_LSE.append(a)
print('Residual LSE',Residual_LSE)

Residual_Gaussian_Elimination = []
for i in range(0, len(Ytest)):
  b = abs(Ytest[i] - y_prediction_gaussian_elimination[i])
  Residual_Gaussian_Elimination.append(b)
print('Residual_Gaussian_Elimination',Residual_Gaussian_Elimination)

"""+ Find the average residual"""

Average_residual_LSE = np.mean(Residual_LSE)
print('Average LSE',Average_residual_LSE)

Average_residual_Gaussian_Elimination = np.mean(Residual_Gaussian_Elimination)
print('Average Gaussian Elimination',Average_residual_Gaussian_Elimination)

"""+ Find the Root Mean Square Error"""

RMSE = np.sqrt(np.square(np.subtract(Ytest,y_prediction))).mean()
print('RMSE LSE', RMSE)

RMSE_gaussian_elimination = np.sqrt(np.square(np.subtract(Ytest,y_prediction_gaussian_elimination))).mean()
print('RMSE Gaussian Elimination',RMSE_gaussian_elimination)

"""# Bibliography

+ Gaussian elimination using NumPy. Retrieved from https://gist.github.com/num3ric/1357315.
+ Thoma, M. Solving linear equations with Gaussian elimination. Retrieved from: https://gist.github.com/num3ric/1357315
"""